[net]
batch = 64
subdivisions = 8
# Training
width = 512
height = 512
channels = 3
momentum = 0.949
decay = 0.0005
angle = 0
saturation = 1.5
exposure = 1.5
hue = .1

learning_rate = 0.0013
burn_in = 1000
max_batches = 500500
policy = steps
steps = 400000,450000
scales = .1,.1

#cutmix=1
mosaic = 1

#:104x104 54:52x52 85:26x26 104:13x13 for 416

[convolutional]
batch_normalize = 1
filters = 16
size = 3
stride = 2
pad = 1
activation = hard-swish

####### 第一个bottleneck集合中只有一个bneck且第一个PW卷积层被省略

[convolutional]
batch_normalize = 1
filters = 16
size = 3
stride = 1
pad = 1
groups = 16
activation = relu

[convolutional]
batch_normalize = 1
filters = 16
size = 1
stride = 1
pad = 1
activation = linear

### 第二个bottleneck集合有两个bottleneck

[convolutional]
batch_normalize = 1
filters = 64
size = 1
stride = 2
pad = 1
activation = relu

[convolutional]
batch_normalize = 1
filters = 64
size = 3
stride = 1
pad = 1
groups = 64
activation = relu

[convolutional]
batch_normalize = 1
filters = 24
size = 1
stride = 1
pad = 1
activation = linear

[convolutional]
batch_normalize = 1
filters = 72
size = 1
stride = 1
pad = 1
activation = relu

[convolutional]
batch_normalize = 1
filters = 72
size = 3
stride = 1
pad = 1
groups = 72
activation = relu

[convolutional]
batch_normalize = 1
filters = 24
size = 1
stride = 1
pad = 1
activation = linear

[shortcut]
from = -4
activation = linear

### 第三个bottleneck集合有3个bottleneck，输出到小目标预测分支 23

[convolutional]
batch_normalize = 1
filters = 72
size = 1
stride = 2
pad = 1
activation = relu

[convolutional]
batch_normalize = 1
filters = 72
size = 5
stride = 1
pad = 1
groups = 72
activation = relu

[se]
squeeze_factor = 4

[convolutional]
batch_normalize = 1
filters = 40
size = 1
stride = 1
pad = 1
activation = linear

[convolutional]
batch_normalize = 1
filters = 120
size = 1
stride = 1
pad = 1
activation = relu

[convolutional]
batch_normalize = 1
filters = 120
size = 5
stride = 1
pad = 1
groups = 120
activation = relu

[se]
squeeze_factor = 4

[convolutional]
batch_normalize = 1
filters = 40
size = 1
stride = 1
pad = 1
activation = linear

[shortcut]
from = -5
activation = linear

[convolutional]
batch_normalize = 1
filters = 120
size = 1
stride = 1
pad = 1
activation = relu

[convolutional]
batch_normalize = 1
filters = 120
size = 5
stride = 1
pad = 1
groups = 120
activation = relu

[se]
squeeze_factor = 4

[convolutional]
batch_normalize = 1
filters = 40
size = 1
stride = 1
pad = 1
activation = linear

[shortcut]
from = -5
activation = linear

### 第四个bottleneck集合中有个4个bottleneck

[convolutional]
batch_normalize = 1
filters = 240
size = 1
stride = 2
pad = 1
activation = hard-swish

[convolutional]
batch_normalize = 1
filters = 240
size = 3
stride = 1
pad = 1
groups = 240
activation = hard-swish

[convolutional]
batch_normalize = 1
filters = 80
size = 1
stride = 1
pad = 1
activation = linear

[convolutional]
batch_normalize = 1
filters = 200
size = 1
stride = 1
pad = 1
activation = hard-swish

[convolutional]
batch_normalize = 1
filters = 200
size = 3
stride = 1
pad = 1
groups = 200
activation = hard-swish

[convolutional]
batch_normalize = 1
filters = 80
size = 1
stride = 1
pad = 1
activation = linear

[shortcut]
from = -4
activation = linear

[convolutional]
batch_normalize = 1
filters = 184
size = 1
stride = 1
pad = 1
activation = hard-swish

[convolutional]
batch_normalize = 1
filters = 184
size = 3
stride = 1
pad = 1
groups = 184
activation = hard-swish

[convolutional]
batch_normalize = 1
filters = 80
size = 1
stride = 1
pad = 1
activation = linear

[shortcut]
from = -4
activation = linear

[convolutional]
batch_normalize = 1
filters = 184
size = 1
stride = 1
pad = 1
activation = hard-swish

[convolutional]
batch_normalize = 1
filters = 184
size = 3
stride = 1
pad = 1
groups = 184
activation = hard-swish

[convolutional]
batch_normalize = 1
filters = 80
size = 1
stride = 1
pad = 1
activation = linear

[shortcut]
from = -4
activation = linear

### 第5个bottleneck集合有2个bottleneck，其输出到中等尺度目标分支 47

[convolutional]
batch_normalize = 1
filters = 480
size = 1
stride = 1
pad = 1
activation = hard-swish

[convolutional]
batch_normalize = 1
filters = 480
size = 3
stride = 1
pad = 1
groups = 480
activation = hard-swish

[se]
squeeze_factor = 4

[convolutional]
batch_normalize = 1
filters = 112
size = 1
stride = 1
pad = 1
activation = linear

[convolutional]
batch_normalize = 1
filters = 672
size = 1
stride = 1
pad = 1
activation = hard-swish

[convolutional]
batch_normalize = 1
filters = 672
size = 3
stride = 1
pad = 1
groups = 672
activation = hard-swish

[se]
squeeze_factor = 4

[convolutional]
batch_normalize = 1
filters = 112
size = 1
stride = 1
pad = 1
activation = linear

[shortcut]
from = -5
activation = linear

### 第6个bottleneck集合中有三个bottleneck，输出到大尺度目标预测分支 61

[convolutional]
batch_normalize = 1
filters = 672
size = 1
stride = 2
pad = 1
activation = hard-swish

[convolutional]
batch_normalize = 1
filters = 672
size = 5
stride = 1
pad = 1
groups = 672
activation = hard-swish

[se]
squeeze_factor = 4

[convolutional]
batch_normalize = 1
filters = 160
size = 1
stride = 1
pad = 1
activation = linear

[convolutional]
batch_normalize = 1
filters = 960
size = 1
stride = 1
pad = 1
activation = hard-swish

[convolutional]
batch_normalize = 1
filters = 960
size = 5
stride = 1
pad = 1
groups = 960
activation = hard-swish

[se]
squeeze_factor = 4

[convolutional]
batch_normalize = 1
filters = 160
size = 1
stride = 1
pad = 1
activation = linear

[shortcut]
from = -5
activation = linear

[convolutional]
batch_normalize = 1
filters = 960
size = 1
stride = 1
pad = 1
activation = hard-swish

[convolutional]
batch_normalize = 1
filters = 960
size = 5
stride = 1
pad = 1
groups = 960
activation = hard-swish

[se]
squeeze_factor = 4

[convolutional]
batch_normalize = 1
filters = 160
size = 1
stride = 1
pad = 1
activation = linear

[shortcut]
from = -5
activation = linear

###########################################################

[convolutional]
batch_normalize = 1
filters = 512
size = 1
stride = 1
pad = 1
activation = relu6

[depthwiseconvolutional]
filters = 1024
stride = 1

[convolutional]
batch_normalize = 1
filters = 512
size = 1
stride = 1
pad = 1
activation = relu6

### SPP ###
[maxpool]
stride = 1
size = 5

[route]
layers = -2

[maxpool]
stride = 1
size = 9

[route]
layers = -4

[maxpool]
stride = 1
size = 13

[route]
layers = -1,-3,-5,-6
### End SPP ###

[convolutional]
batch_normalize = 1
filters = 512
size = 1
stride = 1
pad = 1
activation = relu6

[depthwiseconvolutional]
filters = 1024
stride = 1

[convolutional]
batch_normalize = 1
filters = 512
size = 1
stride = 1
pad = 1
activation = relu6

[convolutional]
batch_normalize = 1
filters = 256
size = 1
stride = 1
pad = 1
activation = relu6

[upsample]
stride = 2

[route]
layers = 47

[convolutional]
batch_normalize = 1
filters = 256
size = 1
stride = 1
pad = 1
activation = relu6

[route]
layers = -1, -3

[convolutional]
batch_normalize = 1
filters = 256
size = 1
stride = 1
pad = 1
activation = relu6

[depthwiseconvolutional]
filters = 512
stride = 1

[convolutional]
batch_normalize = 1
filters = 256
size = 1
stride = 1
pad = 1
activation = relu6

[depthwiseconvolutional]
filters = 512
stride = 1

[convolutional]
batch_normalize = 1
filters = 256
size = 1
stride = 1
pad = 1
activation = relu6

[convolutional]
batch_normalize = 1
filters = 128
size = 1
stride = 1
pad = 1
activation = relu6

[upsample]
stride = 2

[route]
layers = 23

[convolutional]
batch_normalize = 1
filters = 128
size = 1
stride = 1
pad = 1
activation = relu6

[route]
layers = -1, -3

[convolutional]
batch_normalize = 1
filters = 128
size = 1
stride = 1
pad = 1
activation = relu6

[depthwiseconvolutional]
filters = 256
stride = 1

[convolutional]
batch_normalize = 1
filters = 128
size = 1
stride = 1
pad = 1
activation = relu6

[depthwiseconvolutional]
filters = 256
stride = 1

[convolutional]
batch_normalize = 1
filters = 128
size = 1
stride = 1
pad = 1
activation = relu6

##########################

[depthwiseconvolutional]
filters = 256
stride = 1

[convolutional]
size = 1
stride = 1
pad = 1
filters = 18
activation = linear


[yolo]
mask = 0,1,2
anchors = 16, 32, 18, 42, 22, 44, 22, 55, 30, 58, 27, 65, 34, 80, 43, 102, 62, 153
classes = 1
num = 9
jitter = .3
ignore_thresh = .7
truth_thresh = 1
scale_x_y = 1.2
iou_thresh = 0.213
cls_normalizer = 1.0
iou_normalizer = 0.07
iou_loss = ciou
nms_kind = greedynms
beta_nms = 0.6


[route]
layers = -4

[depthwiseconvolutional]
filters = 256
stride = 2

[route]
layers = -1, -16

[convolutional]
batch_normalize = 1
filters = 256
size = 1
stride = 1
pad = 1
activation = relu6

[depthwiseconvolutional]
filters = 512
stride = 1

[convolutional]
batch_normalize = 1
filters = 256
size = 1
stride = 1
pad = 1
activation = relu6

[depthwiseconvolutional]
filters = 512
stride = 1

[convolutional]
batch_normalize = 1
filters = 256
size = 1
stride = 1
pad = 1
activation = relu6

[depthwiseconvolutional]
filters = 512
stride = 1

[convolutional]
size = 1
stride = 1
pad = 1
filters = 18
activation = linear


[yolo]
mask = 3,4,5
anchors = 16, 32, 18, 42, 22, 44, 22, 55, 30, 58, 27, 65, 34, 80, 43, 102, 62, 153
classes = 1
num = 9
jitter = .3
ignore_thresh = .7
truth_thresh = 1
scale_x_y = 1.1
iou_thresh = 0.213
cls_normalizer = 1.0
iou_normalizer = 0.07
iou_loss = ciou
nms_kind = greedynms
beta_nms = 0.6


[route]
layers = -4

[depthwiseconvolutional]
filters = 512
stride = 2

[route]
layers = -1, -37

[convolutional]
batch_normalize = 1
filters = 512
size = 1
stride = 1
pad = 1
activation = leaky

[depthwiseconvolutional]
filters = 1024
stride = 1

[convolutional]
batch_normalize = 1
filters = 512
size = 1
stride = 1
pad = 1
activation = relu6

[depthwiseconvolutional]
filters = 1024
stride = 1

[convolutional]
batch_normalize = 1
filters = 512
size = 1
stride = 1
pad = 1
activation = relu6

[depthwiseconvolutional]
filters = 1024
stride = 1

[convolutional]
size = 1
stride = 1
pad = 1
filters = 18
activation = linear


[yolo]
mask = 6,7,8
anchors = 16, 32, 18, 42, 22, 44, 22, 55, 30, 58, 27, 65, 34, 80, 43, 102, 62, 153
classes = 1
num = 9
jitter = .3
ignore_thresh = .7
truth_thresh = 1
random = 1
scale_x_y = 1.05
iou_thresh = 0.213
cls_normalizer = 1.0
iou_normalizer = 0.07
iou_loss = ciou
nms_kind = greedynms
beta_nms = 0.6